

========== FILE: run_training.py ==========
# run_training.py
"""
Main script to start the training process.
"""
import os
import argparse
from src.train import run_training_pipeline
from src.constants import DATA_PATH, MODEL_SAVE_DIR

def main():
    parser = argparse.ArgumentParser(description="Train Transesterformer Model")
    parser.add_argument('--data_path', type=str, default=DATA_PATH,
                        help='Path to the processed kinetic data CSV file.')
    parser.add_argument('--load_model', type=str, default=None, # Example: 'results/model_checkpoint/best_model.pth'
                        help='Path to a checkpoint file to resume training.')
    parser.add_argument('--force_cpu', action='store_true',
                        help='Force using CPU even if CUDA is available.')

    args = parser.parse_args()

    # --- Device Setup ---
    if args.force_cpu:
        import torch
        from src import constants
        constants.DEVICE = torch.device("cpu")
        # Need to potentially reload modules if DEVICE was used at import time
        # This is tricky, better to set device early or pass it around.
        # For simplicity, we assume constants.py sets DEVICE correctly based on initial check.
        # If forcing CPU, ensure constants.DEVICE reflects this before model/data loading.
        print("Forcing CPU usage.")
        # Update constants directly (use with caution)
        import src.constants
        src.constants.DEVICE = torch.device("cpu")


    # --- Create result directories if they don't exist ---
    if not os.path.exists(MODEL_SAVE_DIR):
        os.makedirs(MODEL_SAVE_DIR)
    if not os.path.exists(os.path.join("results", "figures")): # Ensure figure dir exists
         os.makedirs(os.path.join("results", "figures"))


    # --- Run Training ---
    run_training_pipeline(
        data_path=args.data_path,
        model_load_path=args.load_model
    )

if __name__ == "__main__":
    main()


========== FILE: all_files.py ==========
import os

def dump_all_files_with_contents(root_dir, output_file):
    with open(output_file, 'w', encoding='utf-8') as out:
        for current_dir, _, files in os.walk(root_dir):
            for file in files:
                file_path = os.path.join(current_dir, file)
                rel_path = os.path.relpath(file_path, root_dir)
                
                out.write(f"\n\n========== FILE: {rel_path} ==========\n")
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        contents = f.read()
                        out.write(contents)
                except Exception as e:
                    out.write(f"[ERROR READING FILE: {e}]")

if __name__ == "__main__":
    current_folder = os.path.dirname(os.path.abspath(__file__))
    output_txt = os.path.join(current_folder, "all_files_with_contents.txt")

    dump_all_files_with_contents(current_folder, output_txt)
    print(f"✅ Dump complete. Output saved to: {output_txt}")


========== FILE: all_files_with_contents.txt ==========


========== FILE: data/processed/kinetic_data.csv ==========
# Example CSV data illustrating typical ranges for enzymatic biodiesel production
# Concentrations are illustrative examples in mol/L (assuming initial TG ~1.1 mol/L)
# Conditions reflect common experimental ranges.
experiment_id,time,temperature,methanol_oil_ratio,enzyme_loading,stirring_rate,TG,DG,MG,FAME,Gly,MeOH
WasteOil_45C_5to1_10pct_300rpm,0.0,45.0,5.0,10.0,300.0,1.10,0.00,0.00,0.00,0.00,5.50
WasteOil_45C_5to1_10pct_300rpm,1.0,45.0,5.0,10.0,300.0,0.85,0.15,0.02,0.28,0.09,5.22
WasteOil_45C_5to1_10pct_300rpm,2.0,45.0,5.0,10.0,300.0,0.65,0.25,0.05,0.65,0.22,4.85
WasteOil_45C_5to1_10pct_300rpm,4.0,45.0,5.0,10.0,300.0,0.30,0.20,0.10,1.50,0.50,4.00
WasteOil_45C_5to1_10pct_300rpm,8.0,45.0,5.0,10.0,300.0,0.10,0.08,0.07,2.55,0.85,2.95
WasteOil_45C_5to1_10pct_300rpm,12.0,45.0,5.0,10.0,300.0,0.05,0.04,0.04,2.85,0.95,2.65
WasteOil_45C_5to1_10pct_300rpm,24.0,45.0,5.0,10.0,300.0,0.02,0.02,0.02,3.10,1.03,2.40
WasteOil_55C_4to1_8pct_400rpm,0.0,55.0,4.0,8.0,400.0,1.10,0.00,0.00,0.00,0.00,4.40
WasteOil_55C_4to1_8pct_400rpm,0.5,55.0,4.0,8.0,400.0,0.90,0.12,0.01,0.21,0.07,4.19
WasteOil_55C_4to1_8pct_400rpm,1.0,55.0,4.0,8.0,400.0,0.70,0.20,0.04,0.54,0.18,3.86
WasteOil_55C_4to1_8pct_400rpm,2.0,55.0,4.0,8.0,400.0,0.40,0.22,0.08,1.20,0.40,3.20
WasteOil_55C_4to1_8pct_400rpm,4.0,55.0,4.0,8.0,400.0,0.15,0.10,0.08,2.25,0.75,2.15
WasteOil_55C_4to1_8pct_400rpm,8.0,55.0,4.0,8.0,400.0,0.05,0.04,0.04,2.85,0.95,1.55
WasteOil_55C_4to1_8pct_400rpm,12.0,55.0,4.0,8.0,400.0,0.03,0.02,0.02,3.03,1.01,1.37


========== FILE: data/raw/raw.txt ==========


========== FILE: src/constants.py ==========
# src/constants.py
"""
Defines constants used throughout the project, including physical constants,
species names, expected data columns, and model hyperparameters.
"""

import torch

# --- Physical/Chemical Constants ---
# Approximate Molecular Weights (g/mol) - Adjust based on the specific oil used!
# Assuming average values for a typical vegetable oil (like soybean or sunflower)
MW_OIL = 875.0  # Average MW of Triglyceride (e.g., Triolein C57H104O6)
MW_MEOH = 32.04 # Methanol
MW_FAME = 296.0 # Average MW of Methyl Ester (e.g., Methyl Oleate C19H36O2)
MW_GLY = 92.09  # Glycerol

# --- Species Names & Indices ---
# Ensure these match the columns in your processed data CSV
SPECIES = ['TG', 'DG', 'MG', 'FAME', 'Gly', 'MeOH'] # Order matters!
SPECIES_MAP = {name: i for i, name in enumerate(SPECIES)}
N_SPECIES = len(SPECIES)

# --- Data Columns ---
# Columns expected in the input CSV data/processed/kinetic_data.csv
TIME_COL = 'time'
EXP_ID_COL = 'experiment_id'
CONDITION_COLS = ['temperature', 'methanol_oil_ratio', 'enzyme_loading', 'stirring_rate'] # Input conditions
SPECIES_COLS = SPECIES # Output species concentrations

# --- Model Hyperparameters ---
# Neural ODE settings
NODE_HIDDEN_DIM = 64   # Hidden dimension of the neural network within the ODE function
NODE_N_LAYERS = 2      # Number of hidden layers in the ODE function's network
NODE_ACTIVATION = torch.nn.Tanh # Activation function

# Condition Encoder settings
ENCODER_HIDDEN_DIM = 32
ENCODER_N_LAYERS = 2
ENCODER_OUTPUT_DIM = 16 # Dimension of the encoded condition vector

# Symbolic Kinetics Parameters (Initial guesses or bounds if needed)
# These might be learned or fixed depending on the model variant
INITIAL_K_CAT_GUESS = 1.0 # Example initial guess for catalytic constants
INITIAL_KM_GUESS = 0.1    # Example initial guess for Michaelis constants

# --- Training Hyperparameters ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 16      # Number of experiments per batch
N_EPOCHS = 500       # Total training epochs
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 1e-5  # L2 regularization
ODE_SOLVER = 'dopri5' # Choose from 'dopri5', 'adams', 'rk4' etc. (torchdiffeq)
ODE_TOLERANCE = 1e-4 # Solver tolerance (rtol=atol=ODE_TOLERANCE)
PRINT_FREQ = 10      # Print loss every N epochs
SAVE_FREQ = 50       # Save model checkpoint every N epochs

# Physics-based Loss Weights (tune these carefully)
LAMBDA_MASS_BALANCE = 0.0 # Weight for mass balance penalty (can be 0 if enforced by structure)
LAMBDA_RATE_REG = 1e-6    # Regularization on the magnitude of neural rate corrections

# --- File Paths ---
DATA_PATH = "data/processed/kinetic_data.csv"
MODEL_SAVE_DIR = "results/model_checkpoint/"
FIGURE_SAVE_DIR = "results/figures/"

# --- Stoichiometry ---
# Represents the net change in moles for each species per mole of reaction progress
# Reactions:
# 1: TG + MeOH <-> DG + FAME
# 2: DG + MeOH <-> MG + FAME
# 3: MG + MeOH <-> Gly + FAME
# Columns: TG, DG, MG, FAME, Gly, MeOH
STOICHIOMETRY_MATRIX = torch.tensor([
    [-1,  1,  0,  1,  0, -1], # Reaction 1
    [ 0, -1,  1,  1,  0, -1], # Reaction 2
    [ 0,  0, -1,  1,  1, -1], # Reaction 3
], dtype=torch.float32, device=DEVICE)
N_REACTIONS = STOICHIOMETRY_MATRIX.shape[0]

# --- Normalization ---
# Define means and stds for conditions and species if using normalization
# These should ideally be calculated from the training data
# Example placeholders (replace with actual values calculated in data_loader):
CONDITION_MEANS = torch.tensor([45.0, 4.5, 10.0, 300.0], device=DEVICE) # Temp, Ratio, Enzyme, Stir
CONDITION_STDS = torch.tensor([10.0, 1.5, 5.0, 100.0], device=DEVICE)
SPECIES_MEANS = torch.tensor([0.5, 0.2, 0.1, 0.5, 0.1, 2.0], device=DEVICE) # TG, DG, MG, FAME, Gly, MeOH (mol/L)
SPECIES_STDS = torch.tensor([0.4, 0.2, 0.1, 0.4, 0.1, 1.0], device=DEVICE)
USE_NORMALIZATION = True # Set to False if you don't want normalization



========== FILE: src/model.py ==========
# src/model.py
"""
Defines the Physics‑Guided Neural ODE model (“Transesterformer”),
with robust per‑sample integration and proper tensor shape handling.
"""

import torch
import torch.nn as nn
from torchdiffeq import odeint              # use standard odeint solver

from .constants import (
    N_SPECIES, N_REACTIONS, CONDITION_COLS,
    NODE_HIDDEN_DIM, NODE_N_LAYERS, NODE_ACTIVATION,
    ENCODER_HIDDEN_DIM, ENCODER_N_LAYERS, ENCODER_OUTPUT_DIM,
    STOICHIOMETRY_MATRIX, DEVICE, SPECIES_MAP, LAMBDA_RATE_REG
)

# -------------------------------------------------------------------------
class ConditionEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):
        super().__init__()
        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]
        for _ in range(n_layers - 1):
            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]
        layers.append(nn.Linear(hidden_dim, output_dim))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# -------------------------------------------------------------------------
class SymbolicKinetics(nn.Module):
    def __init__(self, enc_dim):
        super().__init__()
        self.vmax_net  = nn.Linear(enc_dim, N_REACTIONS)
        self.km_tg_net = nn.Linear(enc_dim, 1)
        self.km_dg_net = nn.Linear(enc_dim, 1)
        self.km_mg_net = nn.Linear(enc_dim, 1)
        self.km_meoh   = nn.Linear(enc_dim, N_REACTIONS)
        self.softplus  = nn.Softplus()

    def forward(self, species, enc):
        vmax  = self.softplus(self.vmax_net(enc))
        km_tg = self.softplus(self.km_tg_net(enc))
        km_dg = self.softplus(self.km_dg_net(enc))
        km_mg = self.softplus(self.km_mg_net(enc))
        km_me = self.softplus(self.km_meoh(enc))

        tg = torch.relu(species[:, SPECIES_MAP['TG']])
        dg = torch.relu(species[:, SPECIES_MAP['DG']])
        mg = torch.relu(species[:, SPECIES_MAP['MG']])
        me = torch.relu(species[:, SPECIES_MAP['MeOH']])
        eps = 1e-8

        r1 = vmax[:,0] * tg/(km_tg+tg+eps) * me/(km_me[:,0]+me+eps)
        r2 = vmax[:,1] * dg/(km_dg+dg+eps) * me/(km_me[:,1]+me+eps)
        r3 = vmax[:,2] * mg/(km_mg+mg+eps) * me/(km_me[:,2]+me+eps)
        rates = torch.stack([r1, r2, r3], dim=1)
        return torch.relu(rates)

# -------------------------------------------------------------------------
class NeuralAugmentation(nn.Module):
    def __init__(self, n_species, enc_dim, hidden_dim, n_layers, activation):
        super().__init__()
        layers = [nn.Linear(n_species + enc_dim, hidden_dim), activation()]
        for _ in range(n_layers - 1):
            layers += [nn.Linear(hidden_dim, hidden_dim), activation()]
        layers.append(nn.Linear(hidden_dim, N_REACTIONS))
        self.net = nn.Sequential(*layers)

    def forward(self, species, enc):
        return self.net(torch.cat([species, enc], dim=1))

# -------------------------------------------------------------------------
class TransesterformerODE(nn.Module):
    def __init__(self, enc_dim, hidden_dim, n_layers, activation):
        super().__init__()
        self.symbolic = SymbolicKinetics(enc_dim)
        self.neural   = NeuralAugmentation(N_SPECIES, enc_dim, hidden_dim, n_layers, activation)
        self.S        = STOICHIOMETRY_MATRIX.to(DEVICE)
        self._enc     = None
        self.latest_neural_reg_loss = torch.tensor(0.0, device=DEVICE)

    def set_conditions(self, enc):
        """Store the encoded conditions for this experiment."""
        self._enc = enc

    def forward(self, t, species):
        """
        t: scalar time (ignored)
        species: Tensor of shape (n_species,) or (1, n_species)
        """
        enc = self._enc
        if enc is None:
            raise RuntimeError("Encoded conditions not set before solving ODE.")

        # Ensure species is 2D: (batch=1, n_species)
        squeeze = False
        if species.dim() == 1:
            species = species.unsqueeze(0)
            squeeze = True

        # Broadcast enc to match species batch size if needed
        if enc.dim() == 1:
            enc = enc.unsqueeze(0).expand(species.size(0), -1)
        elif enc.size(0) == 1 and species.size(0) > 1:
            enc = enc.expand(species.size(0), -1)

        # Compute mechanistic and learned rates
        sym_rates = self.symbolic(species, enc)
        neu_rates = self.neural(species, enc)
        self.latest_neural_reg_loss = LAMBDA_RATE_REG * torch.mean(neu_rates ** 2)

        total_rates = torch.relu(sym_rates + neu_rates)  # shape (batch, 3)

        # Compute dC/dt: (batch, 3) @ (3, 6) = (batch, 6)
        dCdt = total_rates @ self.S

        # Return shape (6,) if we started with 1D input
        return dCdt[0] if squeeze else dCdt

# -------------------------------------------------------------------------
class Transesterformer(nn.Module):
    def __init__(
        self,
        n_conditions=len(CONDITION_COLS),
        encoder_hidden_dim=ENCODER_HIDDEN_DIM,
        encoder_output_dim=ENCODER_OUTPUT_DIM,
        encoder_n_layers=ENCODER_N_LAYERS,
        node_hidden_dim=NODE_HIDDEN_DIM,
        node_n_layers=NODE_N_LAYERS,
        node_activation=NODE_ACTIVATION,
        ode_solver="dopri5",
        ode_options=None,
    ):
        super().__init__()
        self.encoder     = ConditionEncoder(n_conditions, encoder_hidden_dim,
                                            encoder_output_dim, encoder_n_layers)
        self.ode_func    = TransesterformerODE(encoder_output_dim,
                                               node_hidden_dim,
                                               node_n_layers,
                                               node_activation)
        self.ode_solver  = ode_solver
        self.ode_options = ode_options.copy() if ode_options else {}
        self.neural_reg_loss = torch.tensor(0.0, device=DEVICE)

    def forward(self, initial_conditions, times, conditions):
        """
        initial_conditions: (batch, n_species)
        times:              (n_times,)
        conditions:         (batch, n_conditions)
        Returns:
            Tensor of shape (batch, n_times, n_species)
        """
        batch_size = initial_conditions.size(0)
        out_list = []

        for i in range(batch_size):
            ic = initial_conditions[i]       # (n_species,)
            cond_i = conditions[i].unsqueeze(0)  # (1, n_conditions)
            enc_i = self.encoder(cond_i)        # (1, enc_dim)
            self.ode_func.set_conditions(enc_i)

            opts = self.ode_options.copy()
            rtol = opts.pop("rtol", 1e-4)
            atol = opts.pop("atol", 1e-4)

            sol = odeint(
                self.ode_func,
                ic,
                times,
                rtol=rtol,
                atol=atol,
                method=self.ode_solver,
                options=opts,
            )  # shape (n_times, n_species)

            out_list.append(sol.unsqueeze(0))   # (1, n_times, n_species)

        result = torch.cat(out_list, dim=0)      # (batch, n_times, n_species)

        # Store final regularization loss
        self.neural_reg_loss = self.ode_func.latest_neural_reg_loss
        return result

# -------------------------------------------------------------------------
if __name__ == "__main__":
    print(f"Testing Transesterformer on {DEVICE}")
    model = Transesterformer(ode_options={"rtol":1e-3,"atol":1e-3}).to(DEVICE)
    B, T = 2, 10
    ic = torch.rand(B, N_SPECIES, device=DEVICE)
    tv = torch.linspace(0,5,T,device=DEVICE)
    conds = torch.rand(B, len(CONDITION_COLS), device=DEVICE)
    with torch.no_grad():
        out = model(ic, tv, conds)
    print("Output shape:", out.shape)  # Expect (B, T, N_SPECIES)


========== FILE: src/train.py ==========
############################  src/train.py  ################################
"""
Training loop, loss calculation, evaluation, and checkpoint logic.
Compatible with run_training.py (expects run_training_pipeline(data_path,
model_load_path=None)).
"""

from __future__ import annotations

# ---------------------------------------------------------------------------
#  Bootstrap so direct execution finds package modules
# ---------------------------------------------------------------------------
import sys
from pathlib import Path

_THIS_DIR = Path(__file__).resolve().parent          # .../transesterformer/src
_ROOT_DIR = _THIS_DIR.parent                         # .../transesterformer
for p in (_THIS_DIR, _ROOT_DIR):
    if str(p) not in sys.path:
        sys.path.insert(0, str(p))

# ---------------------------------------------------------------------------
#  Third‑party / stdlib imports
# ---------------------------------------------------------------------------
import os
import torch
import torch.optim as optim
from tqdm import tqdm

# ---------------------------------------------------------------------------
#  Internal absolute imports
# ---------------------------------------------------------------------------
from src.constants import (
    DEVICE,
    N_EPOCHS,
    LEARNING_RATE,
    WEIGHT_DECAY,
    SAVE_FREQ,
    ODE_SOLVER,
    ODE_TOLERANCE,
    MODEL_SAVE_DIR,
    CONDITION_COLS,
)
from src.model import Transesterformer
from src.data_loader import get_dataloader
from src.utils import (
    denormalize_species,
    denormalize_conditions,
    plot_predictions,
    save_checkpoint,
    load_checkpoint,
    USE_NORMALIZATION,
)

# ---------------------------------------------------------------------------
#  Loss utilities
# ---------------------------------------------------------------------------
def calculate_loss(pred, target, mask, model):
    mask = mask.bool()
    mse = ((pred - target) * mask) ** 2
    data_loss = torch.sum(mse) / torch.sum(mask).clamp(min=1)
    total_loss = data_loss + model.neural_reg_loss
    return total_loss, data_loss

# ---------------------------------------------------------------------------
#  Training epoch
# ---------------------------------------------------------------------------
def train_epoch(model, loader, optimiser, epoch: int):
    model.train()
    running_total = 0.0
    pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{N_EPOCHS} [train]", leave=False)

    for batch in pbar:
        optimiser.zero_grad()
        batch_loss = 0.0
        n_ok = 0

        for exp in batch:
            if exp["times"][0] != 0:
                continue  # skip malformed experiment

            preds = model(
                exp["initial_conditions"].unsqueeze(0),
                exp["times"],
                exp["conditions"].unsqueeze(0),
            )
            loss, _ = calculate_loss(
                preds,
                exp["species_norm"].unsqueeze(0),
                exp["mask"].unsqueeze(0),
                model,
            )
            loss.backward()
            batch_loss += loss.item()
            n_ok += 1

        if n_ok:
            optimiser.step()
            running_total += batch_loss
            pbar.set_postfix({"loss": batch_loss / n_ok})

    pbar.close()
    n_samples = len(loader.dataset)
    return running_total / n_samples if n_samples else float("inf")

# ---------------------------------------------------------------------------
#  Evaluation
# ---------------------------------------------------------------------------
def evaluate(model, loader, epoch: int, n_plot: int = 5):
    model.eval()
    tot = 0.0
    plotted = 0

    with torch.no_grad():
        pbar = tqdm(loader, desc="val", leave=False)
        for batch in pbar:
            batch_loss = 0.0
            n_ok = 0

            for exp in batch:
                if exp["times"][0] != 0:
                    continue

                preds = model(
                    exp["initial_conditions"].unsqueeze(0),
                    exp["times"],
                    exp["conditions"].unsqueeze(0),
                )
                loss, _ = calculate_loss(
                    preds,
                    exp["species_norm"].unsqueeze(0),
                    exp["mask"].unsqueeze(0),
                    model,
                )
                batch_loss += loss.item()
                n_ok += 1

                if plotted < n_plot:
                    t_np = exp["times"].cpu().numpy()
                    true_np = denormalize_species(exp["species_norm"])
                    pred_np = denormalize_species(preds.squeeze(0))
                    cond_vals = (
                        denormalize_conditions(exp["conditions"].unsqueeze(0))
                        .squeeze()
                        .cpu()
                        .numpy()
                    )
                    cond_dict = {k: float(v) for k, v in zip(CONDITION_COLS, cond_vals)}
                    plot_predictions(exp["exp_id"], t_np, true_np, pred_np, cond_dict)
                    plotted += 1

            if n_ok:
                pbar.set_postfix({"val_loss": batch_loss / n_ok})
                tot += batch_loss

        pbar.close()

    n_samples = len(loader.dataset)
    return tot / n_samples if n_samples else float("inf")

# ---------------------------------------------------------------------------
#  Orchestrator (keeps original keyword: model_load_path)
# ---------------------------------------------------------------------------
def run_training_pipeline(data_path: str, model_load_path: str | None = None):
    print(f"Starting training on {DEVICE}")
    print(f"Normalization enabled: {USE_NORMALIZATION}")

    train_loader = get_dataloader(data_path=data_path, shuffle=True)
    val_loader = get_dataloader(data_path=data_path, shuffle=False)

    model = Transesterformer(
        n_conditions=len(CONDITION_COLS),
        ode_solver=ODE_SOLVER,
        ode_options={"rtol": ODE_TOLERANCE, "atol": ODE_TOLERANCE},
    ).to(DEVICE)

    optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    start_epoch, best_val = 0, float("inf")
    if model_load_path and os.path.exists(model_load_path):
        start_epoch, best_val = load_checkpoint(model, optimiser, model_load_path, DEVICE)
        print(f"Resumed from epoch {start_epoch}")

    for epoch in range(start_epoch, N_EPOCHS):
        _ = train_epoch(model, train_loader, optimiser, epoch)
        val_loss = evaluate(model, val_loader, epoch)

        if val_loss < best_val:
            best_val = val_loss
            save_checkpoint(
                model,
                optimiser,
                epoch,
                val_loss,
                os.path.join(MODEL_SAVE_DIR, "best_model.pth"),
            )

        if (epoch + 1) % SAVE_FREQ == 0:
            save_checkpoint(
                model,
                optimiser,
                epoch,
                val_loss,
                os.path.join(MODEL_SAVE_DIR, f"checkpoint_epoch_{epoch+1}.pth"),
            )

    print(f"Training finished. Best validation loss: {best_val:.6f}")

# ---------------------------------------------------------------------------
#  Direct execution helper
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    default_data = (_ROOT_DIR / "data" / "processed" / "kinetic_data.csv").as_posix()
    parser.add_argument("--data", default=default_data)
    parser.add_argument("--ckpt", default=None, dest="model_load_path")
    args = parser.parse_args()

    run_training_pipeline(args.data, args.model_load_path)


========== FILE: src/utils.py ==========
# src/utils.py
"""
Utility functions for data processing, normalization, plotting, and saving.
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd
from .constants import (
    SPECIES, SPECIES_MAP, N_SPECIES, MW_OIL, MW_MEOH, MW_FAME, MW_GLY,
    CONDITION_COLS, SPECIES_COLS, TIME_COL, EXP_ID_COL,
    CONDITION_MEANS, CONDITION_STDS, SPECIES_MEANS, SPECIES_STDS, USE_NORMALIZATION,
    FIGURE_SAVE_DIR, MODEL_SAVE_DIR
)

def normalize_conditions(conditions_df):
    """Normalizes condition variables using pre-defined means and stds."""
    if not USE_NORMALIZATION:
        return torch.tensor(conditions_df[CONDITION_COLS].values, dtype=torch.float32)

    conditions_tensor = torch.tensor(conditions_df[CONDITION_COLS].values, dtype=torch.float32)
    # Ensure means and stds are tensors on the correct device
    means = CONDITION_MEANS.to(conditions_tensor.device)
    stds = CONDITION_STDS.to(conditions_tensor.device)
    # Add small epsilon to stds to prevent division by zero
    normalized_conditions = (conditions_tensor - means) / (stds + 1e-8)
    return normalized_conditions

def denormalize_conditions(normalized_conditions_tensor):
    """Denormalizes condition variables."""
    if not USE_NORMALIZATION:
        return normalized_conditions_tensor

    # Ensure means and stds are tensors on the correct device
    means = CONDITION_MEANS.to(normalized_conditions_tensor.device)
    stds = CONDITION_STDS.to(normalized_conditions_tensor.device)
    conditions = normalized_conditions_tensor * (stds + 1e-8) + means
    return conditions

def normalize_species(species_df):
    """Normalizes species concentrations using pre-defined means and stds."""
    if not USE_NORMALIZATION:
        return torch.tensor(species_df[SPECIES_COLS].values, dtype=torch.float32)

    species_tensor = torch.tensor(species_df[SPECIES_COLS].values, dtype=torch.float32)
    means = SPECIES_MEANS.to(species_tensor.device)
    stds = SPECIES_STDS.to(species_tensor.device)
    normalized_species = (species_tensor - means) / (stds + 1e-8)
    return normalized_species

def denormalize_species(normalized_species_tensor):
    """Denormalizes species concentrations."""
    if not USE_NORMALIZATION:
        return normalized_species_tensor.cpu().numpy() # Return numpy array for plotting

    # Ensure tensor is on CPU for numpy conversion if needed
    normalized_species_tensor = normalized_species_tensor.cpu()
    means = SPECIES_MEANS.cpu()
    stds = SPECIES_STDS.cpu()
    species = normalized_species_tensor * (stds + 1e-8) + means
    return species.numpy()

def calculate_initial_moles(oil_mass_g, methanol_oil_ratio, density_oil=0.92, density_methanol=0.79):
    """
    Calculates initial moles of TG and Methanol based on oil mass and ratio.
    Assumes oil mass is given, calculates volume, then moles.
    Args:
        oil_mass_g (float): Initial mass of oil in grams.
        methanol_oil_ratio (float): Molar ratio of methanol to oil (TG).
        density_oil (float): Density of oil (g/mL).
        density_methanol (float): Density of methanol (g/mL).

    Returns:
        tuple: (initial_moles_tg, initial_moles_methanol, total_volume_L)
               Returns NaNs if inputs are invalid.
    """
    if oil_mass_g <= 0 or methanol_oil_ratio <= 0:
        return np.nan, np.nan, np.nan

    moles_tg = oil_mass_g / MW_OIL
    moles_methanol = moles_tg * methanol_oil_ratio

    volume_oil_mL = oil_mass_g / density_oil
    mass_methanol_g = moles_methanol * MW_MEOH
    volume_methanol_mL = mass_methanol_g / density_methanol

    total_volume_L = (volume_oil_mL + volume_methanol_mL) / 1000.0

    if total_volume_L <= 0:
         return np.nan, np.nan, np.nan

    return moles_tg, moles_methanol, total_volume_L

def convert_mass_fraction_to_molar(df, initial_oil_mass_g, density_oil=0.92, density_methanol=0.79):
    """
    Converts mass fractions (%) in a DataFrame to molar concentrations (mol/L).
    Requires initial oil mass and methanol:oil ratio to estimate total volume.
    Assumes the 'methanol_oil_ratio' column exists in the df for the first time point.
    Adds molar concentration columns to the DataFrame.

    Args:
        df (pd.DataFrame): DataFrame with mass fraction columns (TG, DG, MG, FAME, Gly)
                           and condition columns including 'methanol_oil_ratio'.
        initial_oil_mass_g (float): The initial mass of oil used for this experiment (e.g., 100g).
                                    This might need to be added based on experiment_id.
                                    *** This is a simplification - real volume changes! ***
        density_oil (float): Density of oil (g/mL).
        density_methanol (float): Density of methanol (g/mL).

    Returns:
        pd.DataFrame: DataFrame with added molar concentration columns.
                      Returns original df if conversion fails.
    """
    # Estimate initial moles and volume using the first time point's ratio
    if df.empty or 'methanol_oil_ratio' not in df.columns:
        print("Warning: Missing 'methanol_oil_ratio' or empty dataframe, cannot convert units.")
        return df

    initial_ratio = df['methanol_oil_ratio'].iloc[0]
    moles_tg_init, moles_meoh_init, total_vol_L = calculate_initial_moles(
        initial_oil_mass_g, initial_ratio, density_oil, density_methanol
    )

    if pd.isna(total_vol_L) or total_vol_L <= 0:
        print(f"Warning: Could not calculate initial volume for experiment. Skipping unit conversion.")
        # Add NaN columns so downstream code doesn't break
        for species in SPECIES:
             if species not in df.columns: # Avoid overwriting if already molar
                 df[species] = np.nan
        return df

    # Assume total mass is roughly conserved (approximation!)
    # Use initial oil mass as reference total mass for % calculation
    total_mass_ref = initial_oil_mass_g # Simplification

    # Molecular weights dictionary
    mw = {'TG': MW_OIL, 'DG': MW_OIL - MW_FAME + MW_MEOH, # Approx DG/MG MWs
          'MG': MW_OIL - 2*MW_FAME + 2*MW_MEOH,
          'FAME': MW_FAME, 'Gly': MW_GLY, 'MeOH': MW_MEOH}

    for species in SPECIES:
        if species in df.columns and species != 'MeOH': # Convert species if column exists
             # Check if data is likely percentage (0-100 range)
             is_percent = df[species].max() <= 100.1 and df[species].min() >= -0.1

             if is_percent:
                 mass_species_g = (df[species] / 100.0) * total_mass_ref
                 moles_species = mass_species_g / mw[species]
                 df[species] = moles_species / total_vol_L # Overwrite with mol/L
             # Else: Assume it's already in mol/L or other unit, leave as is
             # More robust checking could be added here

    # Estimate Methanol concentration (difficult, often not measured directly)
    # We can estimate consumption based on FAME produced
    if 'FAME' in df.columns and 'MeOH' not in df.columns:
        initial_meoh_conc = moles_meoh_init / total_vol_L
        # FAME production consumes MeOH stoichiometrically (3 FAME per TG -> 3 MeOH)
        # More accurately: 1 FAME produced consumes 1 MeOH in each step
        moles_fame_produced = df['FAME'] * total_vol_L # FAME conc * vol
        moles_meoh_consumed = moles_fame_produced # Approximation: 1:1 mole consumption per FAME
        current_moles_meoh = moles_meoh_init - moles_meoh_consumed
        df['MeOH'] = np.maximum(0, current_moles_meoh / total_vol_L) # Ensure non-negative

    elif 'MeOH' not in df.columns: # If FAME also not present, fill with NaN
        df['MeOH'] = np.nan

    return df


def plot_predictions(exp_id, times, true_species, pred_species, conditions):
    """
    Plots the predicted vs true species concentrations for a single experiment.

    Args:
        exp_id (str): Identifier for the experiment.
        times (np.ndarray): Time points.
        true_species (np.ndarray): Ground truth species concentrations (n_times, n_species).
        pred_species (np.ndarray): Predicted species concentrations (n_times, n_species).
        conditions (pd.Series or dict): Experimental conditions for labeling plot.
    """
    n_species = true_species.shape[1]
    if n_species != len(SPECIES):
         print(f"Warning: Mismatch in species count for plotting {exp_id}. Expected {len(SPECIES)}, got {n_species}")
         # Try plotting based on available columns if possible
         plot_species_indices = range(min(n_species, len(SPECIES)))
         plot_species_names = SPECIES[:min(n_species, len(SPECIES))]
    else:
         plot_species_indices = range(n_species)
         plot_species_names = SPECIES

    if len(plot_species_names) == 0:
        print(f"Error: No species to plot for {exp_id}")
        return

    n_rows = int(np.ceil(len(plot_species_names) / 3))
    fig, axes = plt.subplots(n_rows, 3, figsize=(15, n_rows * 4), sharex=True)
    axes = axes.flatten() # Flatten to easily iterate

    condition_str = ", ".join([f"{k}={v:.2f}" for k, v in conditions.items()])
    fig.suptitle(f"Experiment: {exp_id}\nConditions: {condition_str}", fontsize=14)

    for i, species_idx in enumerate(plot_species_indices):
        species_name = plot_species_names[i]
        ax = axes[i]
        ax.plot(times, true_species[:, species_idx], 'o-', label=f'True {species_name}', markersize=4)
        ax.plot(times, pred_species[:, species_idx], 'x--', label=f'Predicted {species_name}', markersize=4)
        ax.set_ylabel("Concentration (mol/L)") # Assumes molar units after processing
        ax.set_title(species_name)
        ax.legend()
        ax.grid(True, linestyle='--', alpha=0.6)

    # Add x-axis label to the bottom plots
    for i in range(len(plot_species_names), len(axes)):
         axes[i].set_xlabel("Time (hours)") # Assumes hours
         axes[i].axis('off') # Hide unused subplots

    # Ensure bottom axes have x-label if they are used
    used_axes_indices = np.arange(len(plot_species_names))
    bottom_row_start_index = (n_rows - 1) * 3
    for i in used_axes_indices:
        if i >= bottom_row_start_index:
             axes[i].set_xlabel("Time (hours)") # Assumes hours


    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap

    # Save the figure
    if not os.path.exists(FIGURE_SAVE_DIR):
        os.makedirs(FIGURE_SAVE_DIR)
    filename = f"prediction_{exp_id}.png".replace(" ", "_").replace("/", "_") # Sanitize filename
    filepath = os.path.join(FIGURE_SAVE_DIR, filename)
    try:
        plt.savefig(filepath)
        print(f"Saved prediction plot to {filepath}")
    except Exception as e:
        print(f"Error saving plot {filepath}: {e}")
    plt.close(fig) # Close the figure to free memory


def save_checkpoint(model, optimizer, epoch, loss, filepath):
    """Saves model checkpoint."""
    if not os.path.exists(os.path.dirname(filepath)):
        os.makedirs(os.path.dirname(filepath))
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, filepath)
    print(f"Model checkpoint saved to {filepath}")

def load_checkpoint(model, optimizer, filepath, device):
    """Loads model checkpoint."""
    if not os.path.exists(filepath):
        print(f"Checkpoint file not found: {filepath}")
        return 0, float('inf') # Start from epoch 0, infinite loss

    checkpoint = torch.load(filepath, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f"Loaded checkpoint from {filepath} (Epoch {epoch}, Loss {loss:.4f})")
    return epoch + 1, loss # Return next epoch to start from



========== FILE: src/requirements.txt ==========
torch==2.2.2
numpy<2
pandas
matplotlib
tqdm
torchdiffeq==0.2.3
scipy



========== FILE: src/data_loader.py ==========
# src/data_loader.py
"""
Handles loading, preprocessing, and batching of the kinetic data.
"""

from __future__ import annotations
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

from .constants import (
    DATA_PATH,
    TIME_COL,
    EXP_ID_COL,
    CONDITION_COLS,
    SPECIES_COLS,
    N_SPECIES,
    DEVICE,
    BATCH_SIZE,
    USE_NORMALIZATION,
)
from .utils import (
    normalize_conditions,
    normalize_species,
    convert_mass_fraction_to_molar,
    denormalize_conditions,
    denormalize_species,
)

# expose modules so we can update their globals after computing stats
from . import constants as _const
from . import utils as _utils


class BiodieselKineticsDataset(Dataset):
    """PyTorch Dataset for biodiesel kinetic data."""

    def __init__(self, data_path: str = DATA_PATH, device: torch.device = DEVICE):
        self.data_path = data_path
        self.device = device

        self.experiments = self._load_and_process_data()
        if not self.experiments:
            raise ValueError("No valid experiments loaded. Check data path and format.")

        self._calculate_normalization_stats()

    # ------------------------------------------------------------------ #
    # Loading and basic cleaning                                         #
    # ------------------------------------------------------------------ #
    def _load_and_process_data(self):
        try:
            # -------------- ONLY CHANGE: comment='#' ---------------------
            df = pd.read_csv(
                self.data_path,
                comment="#",          # skip lines that start with '#'
                skip_blank_lines=True
            )
            # -------------------------------------------------------------
            print(f"Loaded data with columns: {df.columns.tolist()}")
        except FileNotFoundError as e:
            raise FileNotFoundError(f"Data file not found at {self.data_path}") from e
        except Exception as e:
            raise RuntimeError(f"Error loading data: {e}") from e

        df = df.sort_values(by=[EXP_ID_COL, TIME_COL])

        required = [EXP_ID_COL, TIME_COL] + CONDITION_COLS + SPECIES_COLS
        missing = [col for col in required if col not in df.columns]
        if missing:
            raise ValueError(f"Missing required columns: {missing}")

        # fill NaNs in conditions
        for col in CONDITION_COLS:
            if df[col].isna().any():
                mean_val = df[col].mean()
                print(f"Filling NaNs in condition '{col}' with mean {mean_val:.2f}")
                df[col].fillna(mean_val, inplace=True)

        print("Assuming species concentrations already molar. Skipping unit conversion.")

        nan_summary = df[SPECIES_COLS].isna().sum()
        if nan_summary.any():
            print("NaNs in species columns will be masked during training:")
            print(nan_summary[nan_summary > 0])

        experiments = []
        grouped = df.groupby(EXP_ID_COL)
        print(f"Processing {len(grouped)} unique experiments...")
        for exp_id, g in grouped:
            if len(g) < 2:
                print(f"Skipping {exp_id}: needs at least two time points.")
                continue

            g = g.sort_values(TIME_COL)

            times = torch.tensor(g[TIME_COL].values, dtype=torch.float32, device=self.device)
            cond_df = g.iloc[[0]][CONDITION_COLS]
            conditions = normalize_conditions(cond_df).squeeze(0).to(self.device)

            raw_species = g[SPECIES_COLS].values
            mask = ~np.isnan(raw_species)
            filled_species = np.nan_to_num(raw_species, nan=0.0)
            species_norm = normalize_species(pd.DataFrame(filled_species, columns=SPECIES_COLS)).to(
                self.device
            )
            mask_tensor = torch.tensor(mask, dtype=torch.bool, device=self.device)

            experiments.append(
                {
                    "exp_id": exp_id,
                    "times": times,
                    "conditions": conditions,
                    "species_raw": torch.tensor(raw_species, dtype=torch.float32, device=self.device),
                    "species_norm": species_norm,
                    "initial_conditions": species_norm[0].clone().detach(),
                    "mask": mask_tensor,
                }
            )

        print(f"Successfully processed {len(experiments)} experiments.")
        return experiments

    # ------------------------------------------------------------------ #
    # Normalization statistics (unchanged)                               #
    # ------------------------------------------------------------------ #
    def _calculate_normalization_stats(self):
        if not USE_NORMALIZATION:
            print("Global normalization disabled — using predefined constants.")
            return

        cond_rows, species_rows = [], []
        for exp in self.experiments:
            cond_rows.append(
                denormalize_conditions(exp["conditions"].unsqueeze(0)).cpu().numpy()
            )
            spec_full = denormalize_species(exp["species_norm"]).reshape(-1, N_SPECIES)
            valid_mask = exp["mask"].cpu().numpy().reshape(-1, N_SPECIES)
            valid_rows = spec_full[valid_mask.all(axis=1)]
            if valid_rows.size:
                species_rows.append(valid_rows)

        if not cond_rows or not species_rows:
            print("Not enough data to recompute normalization stats. Using defaults.")
            return

        cond_arr = np.vstack(cond_rows)
        spec_arr = np.vstack(species_rows)

        cond_means = np.mean(cond_arr, axis=0)
        cond_stds = np.std(cond_arr, axis=0)
        spec_means = np.mean(spec_arr, axis=0)
        spec_stds = np.std(spec_arr, axis=0)

        cond_stds[cond_stds < 1e-8] = 1.0
        spec_stds[spec_stds < 1e-8] = 1.0

        _const.CONDITION_MEANS = torch.tensor(cond_means, dtype=torch.float32, device=self.device)
        _const.CONDITION_STDS = torch.tensor(cond_stds, dtype=torch.float32, device=self.device)
        _const.SPECIES_MEANS = torch.tensor(spec_means, dtype=torch.float32, device=self.device)
        _const.SPECIES_STDS = torch.tensor(spec_stds, dtype=torch.float32, device=self.device)

        _utils.CONDITION_MEANS = _const.CONDITION_MEANS
        _utils.CONDITION_STDS = _const.CONDITION_STDS
        _utils.SPECIES_MEANS = _const.SPECIES_MEANS
        _utils.SPECIES_STDS = _const.SPECIES_STDS

        print("Updated normalization constants from dataset.")
        print("Condition means:", cond_means)
        print("Condition stds :", cond_stds)
        print("Species means  :", spec_means)
        print("Species stds   :", spec_stds)

        # re‑normalize cached tensors
        for exp in self.experiments:
            exp["conditions"] = normalize_conditions(
                pd.DataFrame(
                    [denormalize_conditions(exp["conditions"].unsqueeze(0)).cpu().numpy().squeeze()],
                    columns=CONDITION_COLS,
                )
            ).squeeze(0).to(self.device)

            raw_df = pd.DataFrame(exp["species_raw"].cpu().numpy(), columns=SPECIES_COLS)
            filled = np.nan_to_num(raw_df.values, nan=0.0)
            exp["species_norm"] = normalize_species(pd.DataFrame(filled, columns=SPECIES_COLS)).to(
                self.device
            )
            exp["initial_conditions"] = exp["species_norm"][0].clone().detach()

    # ------------------------------------------------------------------ #
    def __len__(self):
        return len(self.experiments)

    def __getitem__(self, idx):
        return self.experiments[idx]


# ---------------------------------------------------------------------- #
def get_dataloader(
    data_path: str = DATA_PATH,
    batch_size: int = BATCH_SIZE,
    shuffle: bool = True,
    device: torch.device = DEVICE,
):
    dataset = BiodieselKineticsDataset(data_path=data_path, device=device)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda x: x)


if __name__ == "__main__":
    print(f"Loading data on {DEVICE}")
    loader = get_dataloader(batch_size=2, shuffle=False)
    print(f"Dataset size: {len(loader.dataset)} experiments")


========== FILE: src/__pycache__/train.cpython-312.pyc ==========
[ERROR READING FILE: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte]

========== FILE: src/__pycache__/data_loader.cpython-312.pyc ==========
[ERROR READING FILE: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte]

========== FILE: src/__pycache__/utils.cpython-312.pyc ==========
[ERROR READING FILE: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte]

========== FILE: src/__pycache__/constants.cpython-312.pyc ==========
[ERROR READING FILE: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte]

========== FILE: src/__pycache__/model.cpython-312.pyc ==========
[ERROR READING FILE: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte]